{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIi1nEDdZxOS"
      },
      "source": [
        "#  Python for NLP: Working with Facebook FastText Library\n",
        "\n",
        "**FastText for Semantic Similarity**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stM7wGQJYpf8",
        "outputId": "65762f0b-b782-4911-f1f7-04e3b42cbef2"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11686 sha256=652f21abd81d7af54e79735e8a2467a33c6e2168bd2699a872e8be39120dc408\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvbF-N-GZn7q"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzs5GrtKZDSA",
        "outputId": "23cd06a4-9d35-4d5a-cde9-4bda99b41ef8"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models.fasttext import FastText\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "import wikipedia\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkAcJun_ZimF"
      },
      "source": [
        "**Scraping Wikipedia Articles**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGhIuJtQZTSn"
      },
      "source": [
        "artificial_intelligence = wikipedia.page(\"Artificial Intelligence\").content\n",
        "machine_learning = wikipedia.page(\"Machine Learning\").content\n",
        "deep_learning = wikipedia.page(\"Deep Learning\").content\n",
        "neural_network = wikipedia.page(\"Neural Network\").content\n",
        "\n",
        "artificial_intelligence = sent_tokenize(artificial_intelligence)\n",
        "machine_learning = sent_tokenize(machine_learning)\n",
        "deep_learning = sent_tokenize(deep_learning)\n",
        "neural_network = sent_tokenize(neural_network)\n",
        "\n",
        "artificial_intelligence.extend(machine_learning)\n",
        "artificial_intelligence.extend(deep_learning)\n",
        "artificial_intelligence.extend(neural_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0bOd7aZaI2b"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKO9sya7aHyg"
      },
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(document):\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = document.split()\n",
        "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "        tokens = [word for word in tokens if word not in en_stop]\n",
        "        tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "        return preprocessed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyngRJlgaS1M"
      },
      "source": [
        "sent = preprocess_text(\"Artificial intelligence, is the most advanced technology of the present era\")\n",
        "print(sent)\n",
        "\n",
        "\n",
        "final_corpus = [preprocess_text(sentence) for sentence in artificial_intelligence if sentence.strip() !='']\n",
        "\n",
        "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
        "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbVoEJFacSI"
      },
      "source": [
        "**Creating Words Representation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD1EGswHaknW"
      },
      "source": [
        "embedding_size = 60\n",
        "window_size = 40\n",
        "min_word = 5\n",
        "down_sampling = 1e-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3D5BCRJauYl"
      },
      "source": [
        "%%time\n",
        "ft_model = FastText(word_tokenized_corpus,\n",
        "                      size=embedding_size,\n",
        "                      window=window_size,\n",
        "                      min_count=min_word,\n",
        "                      sample=down_sampling,\n",
        "                      sg=1,\n",
        "                      iter=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXWbgSQeayyC"
      },
      "source": [
        "print(ft_model.wv['artificial'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcY5hgHva5bE"
      },
      "source": [
        "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
        "                  for words in ['artificial', 'intelligence', 'machine', 'network', 'recurrent', 'deep']}\n",
        "\n",
        "for k,v in semantically_similar_words.items():\n",
        "    print(k+\":\"+str(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J0lSwUYa-RF"
      },
      "source": [
        "print(ft_model.wv.similarity(w1='artificial', w2='intelligence'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwWO-JAwbDDG"
      },
      "source": [
        "**Visualizing Word Similarities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG1uvvpRbBy_"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "all_similar_words = sum([[k] + v for k, v in semantically_similar_words.items()], [])\n",
        "\n",
        "print(all_similar_words)\n",
        "print(type(all_similar_words))\n",
        "print(len(all_similar_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-u8_cyxgg-d"
      },
      "source": [
        "word_vectors = ft_model.wv[all_similar_words]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "p_comps = pca.fit_transform(word_vectors)\n",
        "word_names = all_similar_words\n",
        "\n",
        "plt.figure(figsize=(18, 10))\n",
        "plt.scatter(p_comps[:, 0], p_comps[:, 1], c='red')\n",
        "\n",
        "for word_names, x, y in zip(word_names, p_comps[:, 0], p_comps[:, 1]):\n",
        "    plt.annotate(word_names, xy=(x+0.06, y+0.03), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6TKUSMOgqn9"
      },
      "source": [
        "**FastText for Text Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWT-VOwAguF3"
      },
      "source": [
        "# Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "yelp_reviews = pd.read_csv(\"/content/drive/My Drive/Colab Datasets/yelp_review_short.csv\")\n",
        "\n",
        "bins = [0,2,5]\n",
        "review_names = ['negative', 'positive']\n",
        "\n",
        "yelp_reviews['reviews_score'] = pd.cut(yelp_reviews['stars'], bins, labels=review_names)\n",
        "\n",
        "yelp_reviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGlwGAABg8s1"
      },
      "source": [
        "Installing FastText\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY6gf327g2F0"
      },
      "source": [
        "!wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS6PzBfjhDp-"
      },
      "source": [
        "!unzip v0.1.0.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c_-t7UbhEGB"
      },
      "source": [
        "cd fastText-0.1.0\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcX9JJZ5hI5d"
      },
      "source": [
        "# check\n",
        "!./fasttext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mX-C8AwhRNr"
      },
      "source": [
        "Text Classification\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ZvEVV2hNZF"
      },
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import csv\n",
        "\n",
        "col = ['reviews_score', 'text']\n",
        "\n",
        "yelp_reviews = yelp_reviews[col]\n",
        "yelp_reviews['reviews_score']=['__label__'+ s for s in yelp_reviews['reviews_score']]\n",
        "yelp_reviews['text']= yelp_reviews['text'].replace('\\n',' ', regex=True).replace('\\t',' ', regex=True)\n",
        "yelp_reviews.to_csv(r'/content/drive/My Drive/Colab Datasets/yelp_reviews_updated.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlFqtW7Ehd5x"
      },
      "source": [
        "yelp_reviews.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSxVlRO2hmp9"
      },
      "source": [
        "!head -n 40000 \"/content/drive/My Drive/Colab Datasets/yelp_reviews_updated.txt\" > \"/content/drive/My Drive/Colab Datasets/yelp_reviews_train.txt\"\n",
        "!tail -n 10000 \"/content/drive/My Drive/Colab Datasets/yelp_reviews_updated.txt\" > \"/content/drive/My Drive/Colab Datasets/yelp_reviews_test.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfjTIl3phqUv"
      },
      "source": [
        "%%time\n",
        "!./fasttext supervised -input \"/content/drive/My Drive/Colab Datasets/yelp_reviews_train.txt\" -output model_yelp_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kos0IyckhuWB"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V51BvqSch54G"
      },
      "source": [
        "!./fasttext test model_yelp_reviews.bin \"/content/drive/My Drive/Colab Datasets/yelp_reviews_test.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlM08jrLh98L"
      },
      "source": [
        "!cat \"/content/drive/My Drive/Colab Datasets/yelp_reviews_train.txt\" | sed -e \"s/\\([.\\!?,’/()]\\)/ \\1 /g\" | tr \"[:upper:]\" \"[:lower:]\" > \"/content/drive/My Drive/Colab Datasets/yelp_reviews_train_clean.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYIhPFcgiBIp"
      },
      "source": [
        "\"/content/drive/My Drive/Colab Datasets/yelp_reviews_test.txt\" | sed -e \"s/\\([.\\!?,’/()]\\)/ \\1 /g\" | tr \"[:upper:]\" \"[:lower:]\" > \"/content/drive/My Drive/Colab Datasets/yelp_reviews_test_clean.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dlbGrSBiD4C"
      },
      "source": [
        "%%time\n",
        "!./fasttext supervised -input \"/content/drive/My Drive/Colab Datasets/yelp_reviews_train_clean.txt\" -output model_yelp_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grg21yciiGYR"
      },
      "source": [
        "!./fasttext test model_yelp_reviews.bin \"/content/drive/My Drive/Colab Datasets/yelp_reviews_test_clean.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go_YKnwWiJpK"
      },
      "source": [
        "%%time\n",
        "!./fasttext supervised -input \"/content/drive/My Drive/Colab Datasets/yelp_reviews_train_clean.txt\" -output model_yelp_reviews -epoch 30 -lr 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rMqeLc-if-M"
      },
      "source": [
        "SRC : https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/"
      ]
    }
  ]
}