== Machine Learning notes ===  

- Intro : a field of AI Algorithms that learn from EXAMPLES and EXPERIENCES instead of traditional HARDCODE et 
RULES


- Application  : Apple and oranges detection

==> Tradictional coding  : too much rules ? 

Solution ML : Model / Classifier and train it to generate the RULES instead of writing them

==> classifier (as function) : takes a data as input (FEATURES) and signs LABEL to it as output(LABELS) 

i.e : input  : Fruit(apple or orange?) => Classifier => apple (if apple chose)
	  ====> : email (spam/mail_ok) => Classifier => spam (if mail_nok)

- Training the Model (CLassifier) 

To TRAIN the Classifier : Supervised Learning or Non suspervised Learning	

* SL : it learns from examples/experiences
* NSL : it learns from events ?   


|--------|			|-----------| 		|-----------|
|Collect |          |Train 	 	|       |Make 		|
|Training|    =>    |Classifier |	=>  |Predictions|
|	Data |          |			|       |			|
|--------|          |-----------|       |-----------|


===========  ML PIPELINE :  in 7 steps ======== 


1. GATHERING / COLLECTING DATA : The more data we collect the more accurate will the model.
- we collect datas to train the model of the system we want to deploy

2. DATA PREPARATION : 

	- Features ? : the input of the system
	- Labels ? : the output of the system 
	
	----------------------------------
	| 		Features 	    | Labels|	
	----------------------------------
	|	x1	|	x2	|	xn	|	y1	|
	----------------------------------
	|	..	|	..	|	..	|	..	|
	----------------------------------
	| xn,m |		|		|	yn,m|
	----------------------------------
	
	- visualization of datas
	- balances, relationship between datas
	- split : training/evaluation(performance of the model)


3. Choosing the MODEL : There are already alot of model created by DataScientists : 
	- For : Music, image, number, text, text based data, linear model (y=ax+b)
	

4. TRAINING (the model : Y = mx+b) :

	- Y : output
	- m : SLope (many m possible, as many features)
	- X : input
	- b : Y-intercept
	
			 [m1,1 m1,2] 
	Weight = [m2,1 m2,2] 
			 [m3,1 m3,2]
			 
			 [b1,1 b1,2] 
	biases = [b2,1 b2,2] 
			 [b3,1 b3,2] 
			 
	Training process
	
|--------|			|-----------| 		|-----------|
|        |          |   Model 	|       | 		    |
|Training|    =>    |   (W,b)   |	=>  |Prediction |
|	Data |          |			|       |			|
|--------|          |-----------|       |-----------|

					|-----------|			||
	/\				| 		    |			\/
	||		<=		|Test/update|	<=
					| (W,b)	    |	
					|-----------|	


Each iteration it's called, a training steps.


/!\ #Residus = Biais ( θ ^ ) ≡ E [ θ ^ ] − θ 
	#Définition —  Si θ ^ est l'estimateur de θ 
	
	
	


5. EVALUATION : after the model is good time to evaluate



|--------|			|-----------| 		|-----------|
|        |          |   Model 	|       | 		    |
|EVALUATION|    =>    |   (W,b)   |	=>  |Prediction |
|	Data |          |			|       |			|
|--------|          |-----------|       |-----------|

					|-----------|			||
	/\				| 		    |			\/
	||		<=		|Test       |	<=
					| (W,b)	    |	
					|-----------|	


This metric allows the model to see the data that has not yet seen.
This is to test how the model might act in the real world


6. PARAMETER TUNING 

	- To improve the training 
	- Repeat the training data several time to increase the accuracy
	- Learning rate : limit of the train / how far we shift the line between two input datas
	- initial conditions : for complexes models (value = 0 ...)
	- Hyperparameters.
	
/!\ : it's important to choose the good parameters to be changed

7. PREDICTIONS : ML uses datas to answer questions

Input : features
Output : Labels 


# References

Josh gordon : https://www.youtube.com/watch?v=cKxRvEZd3Mw&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal

Yufeng Guo/Google Cloud Plateform: https://www.youtube.com/watch?v=nKW8Ndu7Mjw

https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning

